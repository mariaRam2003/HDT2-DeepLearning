{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7a2ba1-6117-4215-ad53-3a91445795a7",
   "metadata": {},
   "source": [
    "## Ejercicio 2 - Repaso Teoría"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62effaa3-b856-4c4e-ba4f-3216cb31eeaf",
   "metadata": {},
   "source": [
    "- - -\n",
    "El objetivo de esta parte es repasar los fundamentos teóricos de Transformers y Attention. Para ello lea el paper\n",
    "“Attention is All You Need”. Luego responda las siguientes preguntas:\n",
    "1. ¿Cuál es la principal innovación de la arquitectura Transformer?\n",
    "2. ¿Cómo funciona el mecanismo de atención del scaled dot-product?\n",
    "3. ¿Por qué se utiliza la atención de múltiples cabezales en Transformer?\n",
    "4. ¿Cómo se incorporan los positional encodings en el modelo Transformer?\n",
    "5. ¿Cuáles son algunas aplicaciones de la arquitectura Transformer más allá de la machine translation?\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebb0d4-9793-4f57-ab91-fa73e65e71eb",
   "metadata": {},
   "source": [
    "##### ¿Cuál es la principal innovación de la arquitectura Transformer?\n",
    "\n",
    "La principal innovación de la arquitectura Transformer es que se basa completamente en mecanismos de atención, eliminando la necesidad de recurrencia y convoluciones, lo que permite una mayor paralelización y eficiencia en el entrenamiento. Esto permite modelar dependencias globales en la secuencia de entrada y salida sin necesidad de procesar los elementos de la secuencia de manera secuencial, lo que mejora significativamente la velocidad y el rendimiento en tareas como la traducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb30dd-bebe-484f-9d66-7eac2efd5820",
   "metadata": {},
   "source": [
    "##### ¿Cómo funciona el mecanismo de atención del scaled dot-product?\n",
    "El mecanismo de atención scaled dot-product calcula la atención como un producto punto entre los vectores de consulta (query) y claves (key), seguido de una normalización (dividiendo por la raíz cuadrada de la dimensión de las claves, $\\sqrt{d_k}$ \n",
    "y aplicando una función softmax para obtener los pesos de atención. Estos pesos se utilizan para calcular una suma ponderada de los valores (value), que es el resultado de la atención."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e5df5-a1d1-4c0c-81fc-47967c57b355",
   "metadata": {},
   "source": [
    "##### ¿Por qué se utiliza la atención de múltiples cabezales en Transformer?\n",
    "La atención de múltiples cabezales (Multi-Head Attention) permite que el modelo enfoque su atención en diferentes partes de la secuencia de entrada simultáneamente. Al dividir la atención en varios \"cabezales\", cada uno aprende a capturar diferentes aspectos de las relaciones entre los elementos de la secuencia, lo que enriquece las representaciones internas del modelo y mejora su capacidad para captar complejas dependencias contextuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247126a-6b21-47af-a52d-9e0de8f9f7ed",
   "metadata": {},
   "source": [
    "##### ¿Cómo se incorporan los positional encodings en el modelo Transformer?\n",
    "Dado que el Transformer no tiene recurrencia ni convoluciones para capturar el orden de los elementos en una secuencia, se incorporan positional encodings para proporcionar información sobre la posición relativa o absoluta de los tokens. Estos positional encodings se suman a las representaciones de los tokens de entrada y están basados en funciones seno y coseno de diferentes frecuencias, lo que permite al modelo aprender relaciones de posición."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42174161-d924-4000-8324-5f3607f88a08",
   "metadata": {},
   "source": [
    "##### ¿Cuáles son algunas aplicaciones de la arquitectura Transformer más allá de la machine translation?\n",
    "Además de la traducción automática, la arquitectura Transformer se ha aplicado exitosamente en tareas como el análisis sintáctico (parsing) del inglés, el resumen de textos, la respuesta a preguntas, la clasificación de secuencias, y más recientemente, en modelos generativos de lenguaje como GPT y BERT que se utilizan para una amplia variedad de tareas en procesamiento de lenguaje natural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
